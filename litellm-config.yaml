# LiteLLM configuration for LM Studio proxy
model_list:
  - model_name: qwen2.5-coder-7b-q5
    litellm_params:
      model: openai/qwen2.5-coder-7b-instruct
      api_base: http://localhost:1234/v1
      api_key: dummy

  - model_name: qwen2.5-7b-instruct
    litellm_params:
      model: openai/qwen2.5-7b-instruct
      api_base: http://localhost:1234/v1
      api_key: dummy

  - model_name: mistral-7b-instruct-v0.3
    litellm_params:
      model: openai/mistralai_-_mistral-7b-instruct-v0.3
      api_base: http://localhost:1234/v1
      api_key: dummy

  - model_name: gpt-oss-20b-gguf
    litellm_params:
      model: openai/gpt-oss-20b
      api_base: http://localhost:1234/v1
      api_key: dummy

  - model_name: starcoderbase-7b
    litellm_params:
      model: openai/bigcode.starcoderbase-7b
      api_base: http://localhost:1234/v1
      api_key: dummy

  # Add hermes when you download it
  # - model_name: hermes-3-8b
  #   litellm_params:
  #     model: openai/hermes-3-llama-3.1-8b
  #     api_base: http://localhost:1234/v1
  #     api_key: dummy

# LiteLLM server settings (no database for simplicity)
general_settings:
  master_key: dummy

# Disable callbacks to avoid dependency issues
litellm_settings:
  success_callback: []
  failure_callback: []